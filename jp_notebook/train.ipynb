{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c324aa5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "8315\n",
      "6384\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os.path import exists\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax, pad\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import torchtext.datasets as datasets\n",
    "import spacy\n",
    "import GPUtil\n",
    "import warnings\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./runs/try')\n",
    "\n",
    "%run Cal_Bleu_batch.ipynb\n",
    "# in Cal_Bleu_batch.ipynb , we have run the next code\n",
    "# %run Model_experiment.ipynb\n",
    "# %run Dataset.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabd05e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyOptimizer(torch.optim.Optimizer):\n",
    "    def __init__(self):\n",
    "        self.param_groups = [{\"lr\": 0}]\n",
    "        None\n",
    "\n",
    "    def step(self):\n",
    "        None\n",
    "\n",
    "    def zero_grad(self, set_to_none=False):\n",
    "        None\n",
    "\n",
    "\n",
    "class DummyScheduler:\n",
    "    def step(self):\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b8b79e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainState:\n",
    "    \"\"\"Track number of steps, examples, and tokens processed\"\"\"\n",
    "\n",
    "    step: int = 0  # Steps in the current epoch\n",
    "    accum_step: int = 0  # Number of gradient accumulation steps\n",
    "    samples: int = 0  # total # of examples used\n",
    "    tokens: int = 0  # total # of tokens processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f87e8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(\n",
    "    data_iter,\n",
    "    model,\n",
    "    loss_compute,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    mode=\"train\",\n",
    "    accum_iter=1,\n",
    "    train_state=TrainState(),\n",
    "):\n",
    "    \"\"\"Train a single epoch\"\"\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    n_accum = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(\n",
    "            batch.src, batch.tgt, batch.src_mask, batch.tgt_mask\n",
    "        )\n",
    "        loss, loss_node = loss_compute(out, batch.tgt_y, batch.ntokens)\n",
    "        # loss_node = loss_node / accum_iter\n",
    "        if mode == \"train\" or mode == \"train+log\":\n",
    "            loss_node.backward()\n",
    "            train_state.step += 1\n",
    "            train_state.samples += batch.src.shape[0]\n",
    "            train_state.tokens += batch.ntokens\n",
    "            if i % accum_iter == 0:\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                n_accum += 1\n",
    "                train_state.accum_step += 1\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "            \n",
    "        if i % 40 == 1 and (mode == \"train\" or mode == \"train+log\"):\n",
    "            lr = optimizer.param_groups[0][\"lr\"]\n",
    "            writer.add_scalar('Loss/train',  loss / batch.ntokens, train_state.accum_step)\n",
    "            writer.add_scalar('LR/train', lr, train_state.accum_step)\n",
    "            \n",
    "            elapsed = time.time() - start\n",
    "            print(\n",
    "                (\n",
    "                    \"Epoch Step: %6d | Accumulation Step: %3d | Loss: %6.2f \"\n",
    "                    + \"| Tokens / Sec: %7.1f | Learning Rate: %6.1e\"\n",
    "                )\n",
    "                % (i, n_accum, loss / batch.ntokens, tokens / elapsed, lr)\n",
    "            )\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "        del loss\n",
    "        del loss_node\n",
    "    return total_loss / total_tokens, train_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ce7efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate(step, model_size, factor, warmup):\n",
    "    \"\"\"\n",
    "    we have to default the step to 1 for LambdaLR function\n",
    "    to avoid zero raising to negative power.\n",
    "    \"\"\"\n",
    "    if step == 0:\n",
    "        step = 1\n",
    "    return factor * (\n",
    "        model_size ** (-0.5) * min(step ** (-0.5), step * warmup ** (-1.5))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc778aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction=\"sum\")\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, true_dist.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b29053d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(x, crit):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    return crit(predict.log(), torch.LongTensor([1])).data\n",
    "\n",
    "\n",
    "def penalization_visualization():\n",
    "    crit = LabelSmoothing(5, 0, 0.1)\n",
    "    loss_data = pd.DataFrame(\n",
    "        {\n",
    "            \"Loss\": [loss(x, crit) for x in range(1, 100)],\n",
    "            \"Steps\": list(range(99)),\n",
    "        }\n",
    "    ).astype(\"float\")\n",
    "\n",
    "    return (\n",
    "        alt.Chart(loss_data)\n",
    "        .mark_line()\n",
    "        .properties(width=350)\n",
    "        .encode(\n",
    "            x=\"Steps\",\n",
    "            y=\"Loss\",\n",
    "        )\n",
    "        .interactive()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57dec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    \"A simple loss compute and train function.\"\n",
    "\n",
    "    def __init__(self, generator, criterion):\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        sloss = (\n",
    "            self.criterion(\n",
    "                x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)\n",
    "            )\n",
    "            / norm\n",
    "        )\n",
    "        return sloss.data * norm, sloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e702d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_worker(\n",
    "    gpu,\n",
    "    ngpus_per_node,\n",
    "    vocab_src,\n",
    "    vocab_tgt,\n",
    "    spacy_de,\n",
    "    spacy_en,\n",
    "    config,\n",
    "):\n",
    "    print(f\"Train worker process using GPU: {gpu} for training\", flush=True)\n",
    "    torch.cuda.set_device(gpu)\n",
    "\n",
    "    pad_idx = vocab_tgt[\"<blank>\"]\n",
    "    d_model = 512\n",
    "#     model = make_model(len(vocab_src), len(vocab_tgt), N=6, d_model=512, d_ff=2048, h=4, dropout=0.1)\n",
    "\n",
    "    #experiment\n",
    "    model = make_model(len(vocab_src), len(vocab_tgt), d_model=256, d_ff=1024, h=4, dropout=0.1)\n",
    "    model.cuda(gpu)\n",
    "    module = model\n",
    "    is_main_process = True\n",
    "    criterion = LabelSmoothing(\n",
    "        size=len(vocab_tgt), padding_idx=pad_idx, smoothing=0.1\n",
    "    )\n",
    "    criterion.cuda(gpu)\n",
    "\n",
    "    train_dataloader, valid_dataloader = create_dataloaders(\n",
    "        gpu,\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=config[\"batch_size\"] // ngpus_per_node,\n",
    "        max_padding=config[\"max_padding\"]\n",
    "    )\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=config[\"base_lr\"], betas=(0.9, 0.98), eps=1e-9\n",
    "    )\n",
    "    lr_scheduler = LambdaLR(\n",
    "        optimizer=optimizer,\n",
    "        lr_lambda=lambda step: rate(\n",
    "            step, d_model, factor=1, warmup=config[\"warmup\"]\n",
    "        ),\n",
    "    )\n",
    "    train_state = TrainState()\n",
    "    \n",
    "    #设置初始随机种子\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    for epoch in range(config[\"num_epochs\"]):\n",
    "        model.train()\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Training ====\", flush=True)\n",
    "        _, train_state = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in train_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            optimizer,\n",
    "            lr_scheduler,\n",
    "            mode=\"train+log\",\n",
    "            accum_iter=config[\"accum_iter\"],\n",
    "            train_state=train_state,\n",
    "        )\n",
    "\n",
    "        GPUtil.showUtilization()\n",
    "        if is_main_process:\n",
    "            file_path = \"%s%.2d.pt\" % (config[\"file_prefix\"], epoch)\n",
    "            torch.save(module.state_dict(), file_path)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"[GPU{gpu}] Epoch {epoch} Validation ====\", flush=True)\n",
    "        model.eval()\n",
    "        sloss, _ = run_epoch(\n",
    "            (Batch(b[0], b[1], pad_idx) for b in valid_dataloader),\n",
    "            model,\n",
    "            SimpleLossCompute(module.generator, criterion),\n",
    "            DummyOptimizer(),\n",
    "            DummyScheduler(),\n",
    "            mode=\"eval\",\n",
    "        )\n",
    "        writer.add_scalar('Loss/valid', sloss, train_state.accum_step)\n",
    "        print(sloss)\n",
    "        torch.cuda.empty_cache()\n",
    "        bleu_score = cal_batch_bleu((Batch(b[0], b[1], pad_idx) for b in valid_dataloader), model, config[\"batch_size\"])\n",
    "        writer.add_scalar('Bleu/valid', bleu_score, train_state.accum_step)\n",
    "\n",
    "    if is_main_process:\n",
    "        file_path = \"%sfinal.pt\" % config[\"file_prefix\"]\n",
    "        torch.save(module.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd13574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train worker process using GPU: 0 for training\n",
      "set model success N2H4D256DFF1024 12ENWSA \n",
      "[GPU0] Epoch 0 Training ====\n",
      "Epoch Step:      1 | Accumulation Step:   1 | Loss:   7.62 | Tokens / Sec:   581.7 | Learning Rate: 5.4e-07\n",
      "Epoch Step:     41 | Accumulation Step:   5 | Loss:   7.57 | Tokens / Sec:  8040.1 | Learning Rate: 1.1e-05\n",
      "Epoch Step:     81 | Accumulation Step:   9 | Loss:   7.53 | Tokens / Sec:  8427.9 | Learning Rate: 2.2e-05\n",
      "Epoch Step:    121 | Accumulation Step:  13 | Loss:   7.40 | Tokens / Sec:  8306.6 | Learning Rate: 3.3e-05\n",
      "Epoch Step:    161 | Accumulation Step:  17 | Loss:   7.27 | Tokens / Sec:  8369.3 | Learning Rate: 4.4e-05\n",
      "Epoch Step:    201 | Accumulation Step:  21 | Loss:   7.16 | Tokens / Sec:  7768.5 | Learning Rate: 5.4e-05\n",
      "Epoch Step:    241 | Accumulation Step:  25 | Loss:   7.02 | Tokens / Sec:  8104.6 | Learning Rate: 6.5e-05\n",
      "Epoch Step:    281 | Accumulation Step:  29 | Loss:   6.94 | Tokens / Sec:  8263.3 | Learning Rate: 7.6e-05\n",
      "Epoch Step:    321 | Accumulation Step:  33 | Loss:   6.74 | Tokens / Sec:  8181.1 | Learning Rate: 8.7e-05\n",
      "Epoch Step:    361 | Accumulation Step:  37 | Loss:   6.66 | Tokens / Sec:  7930.7 | Learning Rate: 9.7e-05\n",
      "Epoch Step:    401 | Accumulation Step:  41 | Loss:   6.51 | Tokens / Sec:  8249.2 | Learning Rate: 1.1e-04\n",
      "Epoch Step:    441 | Accumulation Step:  45 | Loss:   6.47 | Tokens / Sec:  8364.5 | Learning Rate: 1.2e-04\n",
      "Epoch Step:    481 | Accumulation Step:  49 | Loss:   6.35 | Tokens / Sec:  8393.9 | Learning Rate: 1.3e-04\n",
      "Epoch Step:    521 | Accumulation Step:  53 | Loss:   6.11 | Tokens / Sec:  7920.4 | Learning Rate: 1.4e-04\n",
      "Epoch Step:    561 | Accumulation Step:  57 | Loss:   5.94 | Tokens / Sec:  8063.5 | Learning Rate: 1.5e-04\n",
      "Epoch Step:    601 | Accumulation Step:  61 | Loss:   5.77 | Tokens / Sec:  8206.2 | Learning Rate: 1.6e-04\n",
      "Epoch Step:    641 | Accumulation Step:  65 | Loss:   5.59 | Tokens / Sec:  8236.0 | Learning Rate: 1.7e-04\n",
      "Epoch Step:    681 | Accumulation Step:  69 | Loss:   5.52 | Tokens / Sec:  8121.6 | Learning Rate: 1.8e-04\n",
      "Epoch Step:    721 | Accumulation Step:  73 | Loss:   5.28 | Tokens / Sec:  8720.0 | Learning Rate: 1.9e-04\n",
      "Epoch Step:    761 | Accumulation Step:  77 | Loss:   5.31 | Tokens / Sec:  9089.6 | Learning Rate: 2.0e-04\n",
      "Epoch Step:    801 | Accumulation Step:  81 | Loss:   5.11 | Tokens / Sec:  8997.4 | Learning Rate: 2.2e-04\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 77% | 38% |\n",
      "[GPU0] Epoch 0 Validation ====\n",
      "tensor(4.9101, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "def train_model():\n",
    "    config = {\n",
    "        \"batch_size\": 35,\n",
    "        \"num_epochs\": 30,\n",
    "        \"accum_iter\": 10,\n",
    "        \"base_lr\": 1.0,\n",
    "        \"max_padding\": 72,\n",
    "        \"warmup\": 3000,\n",
    "        \"file_prefix\": \"multi30k_model_\",\n",
    "    }\n",
    "    train_worker( 0, 1, vocab_src, vocab_tgt, spacy_de, spacy_en, config)\n",
    "\n",
    "\n",
    "\n",
    "model = train_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b07fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e035f5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
